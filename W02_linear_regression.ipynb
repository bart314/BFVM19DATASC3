{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment: Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment we work on the linear regression of data with one variable. The data we use for this is the profit of a carrier related to the size of a city where that carrier is located. You can imagine that it is useful to know this ratio, because on this basis you can make an informed decision whether you want to open a new branch in a new city (with a certain size). The data is given in the file `data/carrier.pkl`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the data\n",
    "To be able to carry out the exercises, we first load the necessary files and set some variables. Run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dependencies\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#load the data\n",
    "with open('data/carrier.pkl','rb') as f:\n",
    "    data=pickle.load(f) \n",
    "\n",
    "#define the number of observations (m) and the number of features (n)\n",
    "m,n = data.shape\n",
    "\n",
    "#define matrix X, vector y and the initial values of theta (which are zeros)\n",
    "X = np.c_[np.ones(m), data[:, [0]]]\n",
    "y = data[:, [1]]\n",
    "theta = np.zeros( (2, 1) )\n",
    "\n",
    "#pyplot inline \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Inspect the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second step in any machine-learning project is to inspect the data. We can do that by creating an image of the data. The easiest way to do this in this case is to use a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawGraph(data):\n",
    "    \"\"\"\n",
    "    Make a scatter plot of the data with the size of the cities on the x-axis, \n",
    "    the profit of the carrier on the y-axis.\n",
    "    Don't forget to label the axes of the graph (ylabel, xlabel)\n",
    "    \"\"\"\n",
    "    #YOUR CODE HERE\n",
    "\n",
    "    pass\n",
    "\n",
    "drawGraph(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model (<em>cost function</em>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed in the theoretical part, the goal is to minimize the cost function. You minimize this by using <em> gradient descent </em>. It is often useful to keep track of that cost function during the iterations, so that you can plot the different results through your data - for example, you can check whether you have not entered a local minimum. In this assignment we program the cost function further; the following exercise discusses the <em> gradient descent </em>. We ask you specifically to program the cost function yourself instead of using a sklearn module in order to fully understand the principle of the linear regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost function is given by the following equation:\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} ( h_\\theta(x^{(i)}) - y^{(i)} ) ^2$$\n",
    "\n",
    "In which $J(\\theta)$ is the total cost calculated by the current values of $\\theta$; $h_\\theta(x)$ is the hypothesed value of the values (the prediction) and $y$ is the actual value. By adding up and eventually averaging the difference between these two values for each data point, we arrive at the predictive value that the formula has with the current values of $\\theta$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the hypothesis we can use a vectorized implementation. \n",
    "\n",
    "$$ h_\\theta(x) = \\theta^T \\cdot X = \\theta_0 + \\theta_1x $$\n",
    "\n",
    "<p>Since we are looking for a line, we are actually dealing with one parameter (after all, a line is $ y = b + ax $). To make the dimensionality of the training data correspond to $ \\ theta $, we have to add a column of ones.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted values in $ X $, the current values in $ y $ and a theta are given below to the method `compute_cost`; this function must return the value of $ J (\\theta) $. Implement this function; see also the directions in the cell below. Make it work for any size of theta (a vectorial implementation).\n",
    "\n",
    "You should end up with a value of around 32.07."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculated J is 0.0000.\n",
      "This should be around 32.07\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def compute_cost(X, y, theta):\n",
    "    \"\"\"\n",
    "    This method calculates the cost of the current values of theta, that is, the extent to which the\n",
    "    prediction (given the specific value of theta) corresponds to the actual value (that\n",
    "    is given in y).\n",
    "\n",
    "    Every data point in X is multiplied by theta (which dimensions have X and thus theta transposed)\n",
    "    and the result of this is compared with the actual value (so with y). The difference between\n",
    "    these two values are squared and the total of all these squares is divided by it\n",
    "    number of data points to get the average. You must return this average (the variable\n",
    "    J: a number, in short).\n",
    "\n",
    "    A pseudo algorithm could be the following:\n",
    "\n",
    "    1.Determine the number of data points\n",
    "    2.Determine the prediction \n",
    "    3.Calculate the difference between this prediction and the actual value\n",
    "    4.square this difference\n",
    "    5.Add all these squares together and divide by twice the number of data points\n",
    "    \"\"\"\n",
    "\n",
    "    J = 0\n",
    "    \n",
    "    #YOUR CODE HERE\n",
    "\n",
    "    return J\n",
    "\n",
    "J = compute_cost(X, y, theta)\n",
    "print (f\"calculated J is {J:.4f}.\")\n",
    "print (\"This should be around 32.07\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model (Gradient descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you create the method `gradient_descent`. In this method a number of steps are performed, in each step the vector $ \\ theta $ is adjusted according to the formula below.\n",
    "\n",
    "$$ \\theta_j := \\theta_j - \\alpha \\frac{1}{m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})x^{(i)}_j $$\n",
    "\n",
    "In this formula, $\\alpha $ is the learning speed (*learning rate*).\n",
    "\n",
    "If all goes well, every step in this method will cause $ J (\\theta) $ to drop. Note that you edit all $\\theta_j $ at the same time (in this case the size of $ \\theta is $ 2, so every iteration two parameters need to be adjusted). Also, make sure you <em> only </em> change the $ \\theta $: X and y are constant values that don't need to be changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this method, you also need to keep a list that tracks the cost of that specific iteration (the values of J). We use this list in the cell below to actually show the *gradual descent*. Make sure that the method `gradient_descent` returns two things: the final values of theta and the list of costs through the descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values theta=[0. 0.]\n",
      "This should be around (-3.63, 1.16)\n"
     ]
    }
   ],
   "source": [
    "def gradient_descent(X, y, theta, alpha, num_iters):\n",
    "    \"\"\"\n",
    "    In this problem, every parameter of theta num_iter is updated times the optimal values\n",
    "    for these parameters. Per iteration you have to update all parameters of theta.\n",
    "\n",
    "    Each parameter of theta is reduced by the sum of the error of all data points\n",
    "    multiplied by the data point itself (see Blackboard for the corresponding formula).\n",
    "    This sum itself is multiplied by the 'learning rate' alpha.\n",
    "\n",
    "    A possible step-by-step plan would be:\n",
    "    \n",
    "    For every iteration from 1 to num_iters:\n",
    "        1. Determine the prediction for the data point, given the current value of theta\n",
    "        2. Determine the difference between this forecast and the true value\n",
    "        3. Multiply this difference by the ith value of X.\n",
    "        4. Update the ith parameter of theta, namely by decreasing it by\n",
    "        5. alpha times the mean of the sum of the multiplication from 3\n",
    "    \"\"\"\n",
    "    m,n = X.shape\n",
    "    costs = [] \n",
    "    for _ in range(num_iters):\n",
    "        #YOUR CODE HERE\n",
    "        pass\n",
    "\n",
    "    return theta[0], costs\n",
    "\n",
    "alpha = 0.01\n",
    "num_iters = 1500\n",
    "theta = np.zeros( (1,2) )\n",
    "theta, costs = gradient_descent(X, y, theta, alpha, num_iters) \n",
    "print (f\"Values theta={theta}\")\n",
    "print (\"This should be around (-3.63, 1.16)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot the cost against iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def draw_costs(data): \n",
    "    y = np.arange(len(data))\n",
    "    plt.plot(data,y)\n",
    "    plt.ylabel(r'$J(\\theta)$')\n",
    "    plt.xlabel('iterations')\n",
    "    plt.show()\n",
    "    \n",
    "draw_costs(costs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
