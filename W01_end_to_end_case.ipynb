{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science 3 (prediction)\n",
    "\n",
    "## End to end example\n",
    "\n",
    "**Teachers:**\n",
    "\n",
    "* Supervised learning: Bart Barnard (BABA)\n",
    "\n",
    "**Data files:**\n",
    "\n",
    "* heart_failure_clinical_records_dataset.csv\n",
    "* data_description.csv\n",
    "\n",
    "\n",
    "This notebook demonstrates the steps to build a supervised machine learning model. The following steps are performed\n",
    "\n",
    "1. Data is loaded. \n",
    "2. Data is inspected\n",
    "3. Data is visualized to gain more understanding about the data\n",
    "4. Data is prepared for the model to train\n",
    "5. Model is trained on the prepared data\n",
    "6. Model is used to test the working of the model\n",
    "7. Model is evaluated\n",
    "8. Model is improved\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "##  Supervised learning \n",
    "\n",
    "Cardiovascular diseases kill approximately 17 million people globally every year, and they mainly exhibit as myocardial infarctions and heart failures. Heart failure occurs when the heart cannot pump enough blood to meet the needs of the body. Available electronic medical records of patients quantify symptoms, body features, and clinical laboratory test values, which can be used to perform biostatistics analysis aimed at highlighting patterns and correlations otherwise undetectable by medical doctors. Machine learning, can predict patientsâ€™ survival from their data and can individuate the most important features among those included in their medical records[1]. In this we build a machine learning classifier to predict a patient's survival. The goal is to select the most important features for predicting the patient's survival. Data for the analysis is available in `heart_failure_clinical_records_dataset.csv`. The data description is to be found in the table `data_description.csv`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#given code for data description\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "md = pd.read_csv('data/data_description.csv', sep=';')\n",
    "md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Inspect the data\n",
    " \n",
    "\n",
    "Data is loaded. The `time` feature is dropped, since this is not a biometric.  After loading the data the following questions are answered by the aid of creating meaningfull visualizations and overviews. \n",
    "\n",
    "1. Is the dataset balanced? How might that affect the confusion matrix\n",
    "2. Based on the datatype of each feature, are there any features that cannot be used in a logistic regression classifier? \n",
    "3. Is imputation needed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data. Drop the time feature, since this is not a biometric\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('data/heart_failure_clinical_records_dataset.csv')\n",
    "df = df.drop(['time'],axis = 1) \n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# are there any features that cannot be used in a logistic regression classifier? \n",
    "df.info() \n",
    "# all datatypes needs to be numeric to do the calculations. \n",
    "# All datatypes are float or integers and therefore can be used in a classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DEATH_EVENT'].value_counts() \n",
    "# Because of the imbalance of the dataset, \n",
    "# all the methods obtain a better prediction scores on the true negative rate (predicting death), \n",
    "# rather than on the true positive rate\n",
    "\n",
    "survived, dead = df['DEATH_EVENT'].value_counts()\n",
    "print('Number of survived: ', survived)  \n",
    "print('Number of deaths: ', dead) \n",
    "\n",
    "print('\\n')\n",
    "print('% of survived', round(survived / len(df) * 100, 1), '%')\n",
    "print('% of death', round(dead / len(df) * 100, 1), '%')\n",
    "\n",
    "\n",
    "df.DEATH_EVENT.value_counts().plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing data\n",
    "df.isnull().sum() \n",
    "# no missing data, no imputation needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection\n",
    "\n",
    "We create a meaningful visualization to estimate the three most important features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "c = df.corr().abs()\n",
    "sns.heatmap(c)\n",
    "# age, ejection_fraction, serum_creatinine do have most light colors -> strongest contributers?\n",
    "# a model with biometrics is for medical use. It is often expensive to acquire the data, \n",
    "# a simpel model which has an acceptable performance metrics is the most practical. Feature \n",
    "# selection might also improve overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.displot(df, x=\"serum_creatinine\", hue=\"DEATH_EVENT\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a classifier\n",
    "\n",
    "\n",
    "We will build a logistic regression classifier. Next we will evaluate the logistic regression classifier using the confusion matrix. Lastly we create a new patient and predict if the patient will survive. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The df dataframe is the dataframe containing all the raw data (numbers). We will use this to fill the feature matrix $X$ and the vector $y$ with the labeled class. \n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "X = \n",
    "   \\begin{bmatrix} \\\n",
    "    x_1^{(1)}  & x_2^{(1)} & x_3^{(1)} & .. & x_n^{(1)}\\\\\n",
    "    x_1^{(2)}  & x_2^{(2)} & x_3^{(2)} & .. & x_n^{(2)}\\\\ \n",
    "    x_1^{(3)}  & x_2^{(3)} & x_3^{(3)} & .. & x_n^{(3)} \\\\ \n",
    "    .. & .. & .. & .. & ..\\\\ \n",
    "    x_1^{(m)}  & x_2^{(m)} & x_3^{(m)}  & .. & x_n^{(m)}\\\\ \n",
    "   \\end{bmatrix} \n",
    "   \\\n",
    "   %\n",
    "   y = \n",
    "   \\begin{bmatrix} \\\n",
    "   y^{(1)} \\\\\n",
    "   y^{(2)} \\\\ \n",
    "   y^{(3)} \\\\ \n",
    "   .. \\\\ \n",
    "   y^{(m)} \\\\ \n",
    "   \\end{bmatrix} \n",
    "  %\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build model \n",
    "#preprosses data \n",
    "y = np.array(df['DEATH_EVENT'])\n",
    "X = df.iloc[:,[0,1,2,3,4,5,6,7,8,9,10]] #without feature selection\n",
    "\n",
    "#normalise \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def normalize(X):\n",
    "    scalar = StandardScaler()\n",
    "    scalar = scalar.fit(X)\n",
    "    X = scalar.transform(X)\n",
    "    return X\n",
    "\n",
    "X = normalize(X)\n",
    "X.shape\n",
    "\n",
    "#split \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "#train  \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional verify feature weights\n",
    "for i in model.coef_:\n",
    "       for index, j in enumerate(i):        \n",
    "            print(f\"{j:.4}  {list(df.columns.values)[index]}\")\n",
    "# You will see that age, ejection_fraction and serum_creatinine are driving the outcome the most"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(confusion_matrix)\n",
    "print(classification_report(y_test, y_pred))\n",
    "# Accuracy is not that high Indeed negatives are predicted better then positives. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # function to evaluate\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "def plot_learning_curves(model, X_train, y_train, X_val, y_val):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        model:pipeline object\n",
    "        X_train, y_train: training data\n",
    "        X_val, y_val: test data\n",
    "    \"\"\"\n",
    "    train_errors, val_errors = [], []\n",
    "    for m in range(30, len(X_train)):\n",
    "        model.fit(X_train[:m], y_train[:m])\n",
    "        y_train_predict = model.predict(X_train[:m])\n",
    "        y_val_predict = model.predict(X_val)\n",
    "        train_errors.append(mean_squared_error(y_train_predict, y_train[:m]))\n",
    "        val_errors.append(mean_squared_error(y_val_predict, y_val))\n",
    "\n",
    "    plt.plot(np.sqrt(train_errors), \"r-+\", linewidth=1, label=\"training data\")\n",
    "    plt.plot(np.sqrt(val_errors), \"b-\", linewidth=1, label=\"validation data\")\n",
    "    plt.legend(loc=\"upper right\", fontsize=10)   \n",
    "    plt.xlabel(\"Training set size\", fontsize=10) \n",
    "    plt.ylabel(\"RMSE\", fontsize=10)     \n",
    "    # compare accuracy train versus test to access overfit \n",
    "    print(f'test  acc: {model.score(X_val, y_val)}')\n",
    "    print(f'train acc: {model.score(X_train, y_train)}')\n",
    "    \n",
    "    \n",
    "plot_learning_curves(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and or new model with feature selection \n",
    "y = np.array(df['DEATH_EVENT'])\n",
    "X = df.iloc[:,[0,4,7]]\n",
    "#normalise\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def normalize(X):\n",
    "    scalar = StandardScaler()\n",
    "    scalar = scalar.fit(X)\n",
    "    X = scalar.transform(X)\n",
    "    return X\n",
    "\n",
    "X = normalize(X)\n",
    "#split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "#train LogisticRegression\n",
    "new_model = LogisticRegression()\n",
    "new_model.fit(X_train, y_train)\n",
    "# evaluate the model. Is your assumption of contribution features correct based on the outcome?\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = new_model.predict(X_test)\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(confusion_matrix)\n",
    "print(classification_report(y_test, y_pred))\n",
    "# optional\n",
    "\n",
    "print(new_model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"survivalrate: {round(new_model.predict_proba([[25, 25, 2.0]])[0][0], 3)}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### improving the model\n",
    "\n",
    "What might be another classifier suitable for the dataset and the problem? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 Decision trees often perform well on imbalanced datasets because their hierarchical structure allows \n",
    "#  them to learn signals from both classes.\n",
    "#2 cross validation is advisable because of the small sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier, VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# improve model for instance with AdaBoost (applicabl for binary classes)\n",
    "adb = AdaBoostClassifier(LogisticRegression(), n_estimators = 10, learning_rate = 1)\n",
    "adb.fit(X_train, y_train)\n",
    "\n",
    "y_pred = adb.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(confusion_matrix)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
